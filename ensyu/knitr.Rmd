---
title: "Wine"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Random Forest

データ型を確認するとqualityがintとして認識されてしまっていてこのままではうまくいかないのでfactorに変換(回帰だったら数値)
一度文字にしてからfactorにするのが扱いやすくておすすめ
randomForest(mtryはsqrt(変数数)で決めてる)

```{r wine}
library(randomForest)

test <- read.csv("wine_white_test.csv")
dat <- read.csv("wine_white_train.csv")
str(dat)
test[,12] <- as.factor(as.character(test[,12]))
dat[,12] <- as.factor(as.character(dat[,12]))


rf <- randomForest(quality~., data=dat, importance=T)
rf.pre <- predict(rf,test)
t.rf <- table(as.character(test$quality), rf.pre)
(c.rf <- sum(diag(t.rf))/sum(t.rf))
par(mfrow=c(1,1))
```

## 変数の重要性
```{r pressure, echo=FALSE}
plot(rf) #classエラーと全体エラー
importance(rf)
varImpPlot(rf)
```

左がOOBエラー減らす変数で右がジニ係数を減らす＝決定木の上の方にある変数
両方で上にある変数が特に重要と考えられる



## 決定木

```{r rpart}
library(rpart) #決定木のパッケージ
library(partykit)
(dt <- rpart(quality~.,data=dat))
```

決定木の表示（indexからplotまで繰り返すと結果がたまに変わる
決定木は再現性に乏しいため（データロバストでない）
でもアンサンブル学習ではこれが良い方向に働く

###予測
```{r predict, echo=FALSE}
plot(as.party(dt))

#予測
predict(dt, test)
res <- predict(dt, test, type="class")
(t <- table(res,test$quality))
(cr <- sum(diag(t)/sum(t))) #正答率
```
